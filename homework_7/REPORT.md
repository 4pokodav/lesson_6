## 1. Введение

### Описание задачи
В рамках данной работы проводится сравнение производительности моделей ResNet-18, конвертированных в различные форматы (TorchScript, ONNX, Torch-TensorRT), на различных размерах изображений. Цель — определить наиболее эффективный формат и конфигурацию для инференса.

### Методы оптимизации
Используются следующие методы оптимизации:
- Квантование до `fp16`
- Использование `torch-tensorrt` для ускорения инференса
- Перевод модели в формат ONNX для тестирования на CPU

### Ожидаемые результаты
- Повышение скорости инференса при использовании `torch-tensorrt`
- Снижение использования видеопамяти при использовании ONNX
- Явное превосходство `fp16` над `fp32` по скорости
- Визуальное и табличное сравнение эффективности всех подходов

## 2. Методология

### Экспериментальная установка
Тестирование проводилось на компьютере со следующей конфигурацией:
- Процессор: Intel Core i5-9600KF
- Видеокарта: MSI GeForce RTX 4060, 8 ГБ GDDR6

### Параметры тестирования
Для оценки производительности использовались различные размеры батча (32, 40, 48, 56, 64) и входные изображения размером 224×224, 256x256, 384x384, 512x512  пикселя. Тестирование проводилось как на CPU, так и на GPU.

### Методы измерения
Производительность измерялась по метрике FPS (кадров в секунду) и среднему времени инференса одного батча. Для замеров использовались встроенные средства cuda.Event (на GPU) и модуль time (на CPU).

3. Результаты

**Таблица бенчмарка с размером изображений 224x224:**

![Table 224x224](https://github.com/4pokodav/lesson_6/raw/main/homework_7/plots/224.png)

**Таблица бенчмарка с размером изображений 256x256:**

![Table 256x256](https://github.com/4pokodav/lesson_6/raw/main/homework_7/plots/256.png)

**Таблица бенчмарка с размером изображений 384x384:**

![Table 384x384](https://github.com/4pokodav/lesson_6/raw/main/homework_7/plots/384.png)

**Таблица бенчмарка с размером изображений 512x512:**

![Table 512x512](https://github.com/4pokodav/lesson_6/raw/main/homework_7/plots/512.png)

## 4. Обсуждение

### Выводы о выборе оптимального подхода:

- Для задач с максимальной требовательностью к скорости оптимально использовать PyTorch с fp16 на GPU, либо Torch-TensorRT, который предлагает более эффективное использование памяти при близкой производительности.
- Если ограничения по памяти критичны, стоит рассмотреть ONNX с fp16, принимая снижение скорости.
- Использование fp32 оправдано, если точность важнее скорости и ресурсов, но в большинстве production-задач fp16 предпочтительнее.

### Рекомендации для production использования:

- Использовать PyTorch с fp16 на CUDA для максимальной производительности и эффективности.
- Для среды с ограниченной памятью — Torch-TensorRT, поскольку он существенно снижает потребление GPU-памяти при сохранении высокой скорости.
- Внедрять ONNX-модели при необходимости легковесного решения с ограниченным ресурсом памяти, особенно если скорость не критична.
- Избегать запуска моделей на CPU в production для задач с высокими требованиями к скорости.

## 5. Заключение

### Выводы:
- torch-tensorrt показывает лучшие результаты на CUDA при любом размере.
- Увеличение размера изображения пропорционально уменьшает скорость и увеличивает память.
- При использовании ONNX наблюдается низкое потребление памяти, но более медленный инференс по сравнению с torch/torch-tensorrt.
- FP16 дает значительный прирост производительности по сравнению с FP32.
- CPU-инференс для ONNX существенно проигрывает по скорости, особенно при увеличении размера изображений.
